{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload source files automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "\n",
    "from event_clustering.preprocessing import *\n",
    "from event_clustering.clustering import *\n",
    "from event_clustering.postprocessing import *\n",
    "from event_clustering.process_mining import *\n",
    "\n",
    "from sklearn.cluster import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.metrics.cluster as cluster_metrics\n",
    "\n",
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "#http://pm4py.pads.rwth-aachen.de/documentation/conformance-checking/evaluation-log-model/\n",
    "from pm4py.evaluation import factory as evaluation_factory\n",
    "\n",
    "import scipy.cluster.hierarchy as sch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Encoded Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the column name map to suit your dataset\n",
    "column_name_map = {\n",
    "    'timestamp': 'time:timestamp',\n",
    "    'caseid' : 'case:id',\n",
    "    'eventname' : 'concept:name',\n",
    "    'resource' : 'org:resource',\n",
    "    'role' : 'org:role',\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature names PREFIXES, the evaluation automatically includes all columns for each prefix\n",
    "feature_names = [\n",
    "    'concept:name',\n",
    "    'org:role',\n",
    "    'feature_timedif_to',\n",
    "    'feature_time_',\n",
    "    'feature_position',\n",
    "    'feature_window'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiments with indeces of feature names to be used\n",
    "experiments = [\n",
    "    [0],\n",
    "    [0,1],\n",
    "    [0,2],\n",
    "    [0,1,2],\n",
    "    [0,3],\n",
    "    [0,1,3],\n",
    "    [0,1,2,3]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data/'\n",
    "file_name = 'DomesticDeclarations'\n",
    "df = preprocess(load(data_folder + file_name + '.xes'), column_name_map)\n",
    "df_encoded = pd.read_csv(data_folder + file_name + '_encoded.csv')\n",
    "original_df_columns = df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_optimal_clusters(df_encoded, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cluster number depending on experiment above\n",
    "cluster_nr = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in experiments:\n",
    "    \n",
    "    selected_feature_names = []\n",
    "    column_names = []\n",
    "    for name_index in experiment:\n",
    "        feature_name = feature_names[name_index]\n",
    "        selected_feature_names.append(feature_name.replace(\":\", \"\"))\n",
    "        column_names.extend([x for x in df_encoded.columns if feature_name in x])\n",
    "        \n",
    "    experiment_name = file_name + '_exp_' + '+'.join(selected_feature_names)\n",
    "    \n",
    "    df_experiment = df_encoded[column_names]\n",
    "    \n",
    "    experiment_results_path_prefix = 'results/' + experiment_name\n",
    "    \n",
    "    km = MiniBatchKMeans(n_clusters=cluster_nr, init_size=1024, batch_size=2048, random_state=20)\n",
    "    cluster_labels = km.fit_predict(df_experiment)\n",
    "    \n",
    "    np.save(experiment_results_path_prefix + '_cluster_labels', cluster_labels)\n",
    "    \n",
    "     #plot pca\n",
    "    plot_pca(df_experiment, cluster_labels, experiment_results_path_prefix + '_pcaplot')\n",
    "    \n",
    "    # Silhouette\n",
    "    silhouette_path = experiment_results_path_prefix + \"_silhouette.txt\"\n",
    "    if os.path.exists(silhouette_path):\n",
    "        os.remove(silhouette_path)\n",
    "    silhouette = metrics.silhouette_score(df_experiment, cluster_labels)\n",
    "    f = open(silhouette_path, \"x\")\n",
    "    f.write(str(silhouette))\n",
    "    f.close()\n",
    "             \n",
    "    # Completeness\n",
    "    completeness_path = experiment_results_path_prefix + \"_completeness.txt\"\n",
    "    if os.path.exists(completeness_path):\n",
    "        os.remove(completeness_path)\n",
    "    completness = metrics.completeness_score(df[column_name_map['eventname']], cluster_labels)\n",
    "    f = open(completeness_path, \"x\")\n",
    "    f.write(str(completness))\n",
    "    f.close()\n",
    "    \n",
    "    abstracted_df = df.copy()\n",
    "    abstracted_df[column_name_map['eventname']] = ['c_' + str(x) for x in cluster_labels]\n",
    "    merge_subsequent_same_events(abstracted_df, column_name_map['caseid'], column_name_map['eventname'])\n",
    "                                                 \n",
    "    # write abstracted dataframe as csv file\n",
    "    filepath_abstracted = data_folder + experiment_name + '_abstracted.csv'\n",
    "    write_to_csv(abstracted_df, filepath_abstracted, index=False)\n",
    "    \n",
    "    # read logs with pm4py                                           \n",
    "    original_log = read_as_log_xes(data_folder + file_name + '.xes')\n",
    "    abstracted_log = read_as_log_csv(filepath_abstracted)\n",
    "                                                 \n",
    "    # Inductive Miner\n",
    "    net_ind_orig, im_ind_orig, fm_ind_orig = inductive_miner.apply(original_log)\n",
    "    net_ind, im_ind, fm_ind = inductive_miner.apply(abstracted_log)\n",
    "                                                 \n",
    "    visualize_as_petri_net(net_ind_orig, im_ind_orig, fm_ind_orig, experiment_results_path_prefix + '_petrinet.svg')\n",
    "    visualize_as_petri_net(net_ind, im_ind, fm_ind, experiment_results_path_prefix + '_petrinet_abstracted.svg')\n",
    "               \n",
    "    model_metrics_original = evaluation_factory.apply(original_log, net_ind_orig, im_ind_orig, fm_ind_orig)\n",
    "    model_metrics_abstracted = evaluation_factory.apply(abstracted_log, net_ind, im_ind, fm_ind)\n",
    "    \n",
    "    model_metrics_path = experiment_results_path_prefix + \"_model_metrics.txt\"\n",
    "    if os.path.exists(model_metrics_path):\n",
    "        os.remove(model_metrics_path)\n",
    "    f = open(model_metrics_path, \"x\")\n",
    "    f.write(str(model_metrics_original))\n",
    "    f.close()\n",
    "    \n",
    "    model_metrics_abstracted_path = experiment_results_path_prefix + \"_model_metrics_abstracted.txt\"\n",
    "    if os.path.exists(model_metrics_abstracted_path):\n",
    "        os.remove(model_metrics_abstracted_path)\n",
    "    f = open(model_metrics_abstracted_path, \"x\")\n",
    "    f.write(str(model_metrics_abstracted))\n",
    "    f.close()                                                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cluster labels \"manually\"\n",
    "# group by cluster label and aggregate concept:name\n",
    "\n",
    "# change experiment_results_path_prefix if you want to use the cluster labels of another experiment\n",
    "cluster_labels = np.load(experiment_results_path_prefix + '_cluster_labels.npy')\n",
    "df2 = df.copy()\n",
    "df2['cluster_labels'] = cluster_labels\n",
    "eval_manually = df2.groupby('cluster_labels')[column_name_map['eventname']].apply(set)\n",
    "eval_manually"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
